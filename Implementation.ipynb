{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3a54310-4a49-4657-b0ef-c8945c764233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2766595f-dc9e-45ab-95fd-474fbda863c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4392cf52-866c-421a-816c-0807d344c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilgpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "437c945e-8c2a-419f-9074-90826d98a2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59e20344-a953-40ce-a94e-5515a76e7005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1e828bcf134471b0310a1e1035de5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3a47b8e57045f49df92e83954b7171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=model_dtype,  # âœ… FIXED: was torch_dtype\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cda890b8-5ca2-4299-b517-e694855e7c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model on cpu\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "print(f\"âœ… Model on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a840bc5-403e-4538-8601-9fcc969a43ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"distilgpt2\",\n",
    "    torch_dtype=torch.float32,  # Legacy parameter for safety\n",
    "    device=-1,  # Force CPU\n",
    "    max_length=40,\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05c0ceb2-87ac-4615-969c-f745521d45b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d817fff-66d7-4ad0-b311-6d803bc88b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤–: Hello, AI!â€Œ The last of the previous games from th...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = llm(\"Hello, AI!\")\n",
    "print(f\"ðŸ¤–: {response[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a94342c9-c67b-4aa3-af53-135fcebbc4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Prompt template created:\n",
      "Template preview: You are a helpful AI assistant specialized in technical explanations.\n",
      "\n",
      "Instructi...\n",
      "Input variables: ['question']\n",
      "\n",
      "ðŸ”§ Building LLMChain...\n",
      "âœ… LangChain integration complete!\n",
      "Chain components: LLM + 1 prompt variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liutianxing/anaconda3/envs/llm_env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Create sophisticated prompt template\n",
    "prompt_template = \"\"\"You are a helpful AI assistant specialized in technical explanations.\n",
    "\n",
    "Instructions:\n",
    "- Keep answers clear and concise (2-3 sentences maximum)\n",
    "- Use simple language suitable for beginners\n",
    "- Focus on core concepts, avoid jargon\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "# Define the prompt template\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"question\"]\n",
    ")\n",
    "\n",
    "print(\"ðŸ“ Prompt template created:\")\n",
    "print(f\"Template preview: {prompt_template[:80]}...\")\n",
    "print(f\"Input variables: {PROMPT.input_variables}\")\n",
    "\n",
    "# Create LangChain\n",
    "print(\"\\nðŸ”§ Building LLMChain...\")\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT,\n",
    "    verbose=True  # Show chain execution for demo\n",
    ")\n",
    "\n",
    "print(\"âœ… LangChain integration complete!\")\n",
    "print(f\"Chain components: LLM + {len(PROMPT.input_variables)} prompt variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e915952-f599-4450-aa6f-d6f604b7759c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
