{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58908f73-4527-43e2-8b2f-dcafb5c5186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM,pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b2d3e-2359-45db-b627-998fea7392e2",
   "metadata": {},
   "source": [
    "Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a46559d-9921-4836-8e7d-1af16ec75b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44b29cd5-186c-4e47-a57b-52a1e1dae669",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"distilgpt2\"\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a906867-b987-49b2-ad9f-30a8a5e9d5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5bddee60-a7fe-4f80-97ef-dba82810ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.float32,\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ed48432-df29-40f0-b8d9-1a47c7242588",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "688cb938-1044-42dc-8260-686215983ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer= tokenizer,\n",
    "    max_new_tokens=60,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    repetition_penalty=1.1,\n",
    "    device=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44dde0e9-2c16-41e7-97c9-506921e30b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "85b73309-b6ba-4aab-aaa4-11e3dfd2aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_time= time.time() -start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c772c83-41a1-42e1-b2d3-e9720d0ade6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "myquestion='''\n",
    "what is the capital city of china?\n",
    "'''\n",
    "test_start = time.time()\n",
    "test_response = llm(myquestion)\n",
    "test_time = time.time() - test_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b22565a-92c4-4a1d-b9b1-ff13fa807218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: \n",
      "what is the capital city of china?\n",
      "\"The capital city in China has a rich history. It was once an important place for Chinese people to live, learn and make their living.\"...\n"
     ]
    }
   ],
   "source": [
    "print ( f\"Sample: {test_response}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fe3279-2c49-4b3d-9e80-9866a78385df",
   "metadata": {},
   "outputs": [],
   "source": [
    "LangChain Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71021dbd-4498-491a-af6e-020b81b9fec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template preview: You are an AI assistant.\n",
      "\n",
      "Instructions:\n",
      "- keep answers clear\n",
      "- Use simple langu...\n",
      "input variables: ['question']\n"
     ]
    }
   ],
   "source": [
    "prompt_template =\"\"\" You are an AI assistant.\n",
    "\n",
    "Instructions:\n",
    "- keep answers clear\n",
    "- Use simple language\n",
    "- Focus on core concepts\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables =[\"question\"]\n",
    ")\n",
    "\n",
    "print (f\"Template preview:{prompt_template[:80]}...\")\n",
    "print (f\"input variables: {PROMPT.input_variables}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "820bd1ab-3960-42c4-8a74-cce2bc85fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "# Modern chain (no deprecation warning)\n",
    "modern_chain = PROMPT | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ffaa9350-efa4-4713-a2ca-a55a6a391aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain components: LLM +1 prompt variables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liutianxing/anaconda3/envs/llm_env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "chain =LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=PROMPT,\n",
    "    verbose=True\n",
    ")\n",
    "print(f\"Chain components: LLM +{len(PROMPT.input_variables)} prompt variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf422a1-206d-497b-9982-b6eb5832cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Build interactive Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd5d5952-b979-4423-8b46-c96f79f73dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q1:What is AI\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are an AI assistant.\n",
      "\n",
      "Instructions:\n",
      "- keep answers clear\n",
      "- Use simple language\n",
      "- Focus on core concepts\n",
      "\n",
      "User Question: What is AI\n",
      "\n",
      "Assistant:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liutianxing/anaconda3/envs/llm_env/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "(0.7s) You are an AI assistant.\n",
      "\n",
      "Instructions:\n",
      "- keep answers clear...\n",
      "\n",
      "Q2:What is Python\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are an AI assistant.\n",
      "\n",
      "Instructions:\n",
      "- keep answers clear\n",
      "- Use simple language\n",
      "- Focus on core concepts\n",
      "\n",
      "User Question: What is Python\n",
      "\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "(0.7s) You are an AI assistant.\n",
      "\n",
      "Instructions:\n",
      "- keep answers clear...\n",
      "\n",
      "Q3:What is machine learning\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are an AI assistant.\n",
      "\n",
      "Instructions:\n",
      "- keep answers clear\n",
      "- Use simple language\n",
      "- Focus on core concepts\n",
      "\n",
      "User Question: What is machine learning\n",
      "\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "(0.6s) You are an AI assistant.\n",
      "\n",
      "Instructions:\n",
      "- keep answers clear...\n"
     ]
    }
   ],
   "source": [
    "def run_demo():\n",
    "    questions=[\n",
    "        \"What is AI\",\n",
    "        \"What is Python\",\n",
    "        \"What is machine learning\"\n",
    "    ]\n",
    "    for i,q in enumerate(questions,1):\n",
    "        print(f\"\\nQ{i}:{q}\")\n",
    "        start = time.time()\n",
    "        response=chain.run(q)\n",
    "        end=time.time()\n",
    "        clean = response.split(\"Answer:\")[-1].strip()\n",
    "        if len(clean) >60:\n",
    "            clean = clean[:60] + \"...\"\n",
    "        print(f\"({end-start:.1f}s) {clean}\")\n",
    "\n",
    "run_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783305d5-0e09-4c4b-9a38-5f5e748944db",
   "metadata": {},
   "source": [
    "An advanced chatbot which i did not show it in the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213765b-eb8e-411e-b498-26125aaa656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simplified Interactive Chatbot (Demo Version)\n",
    "\n",
    "class SimpleChatbot:\n",
    "    \"\"\"Streamlined chatbot for assignment demo - minimal typing required\"\"\"\n",
    "    \n",
    "    def __init__(self, chain, model_name):\n",
    "        self.chain = chain\n",
    "        self.model_name = model_name\n",
    "        self.conversation_count = 0\n",
    "        self.demo_questions = [\n",
    "            \"What is machine learning?\",\n",
    "            \"Explain neural networks simply\",\n",
    "            \"What is LangChain?\",\n",
    "            \"How does Python work?\",\n",
    "            \"What is quantum computing?\"\n",
    "        ]\n",
    "    \n",
    "    def show_header(self):\n",
    "        \"\"\"Clean header for video demo\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\" SIMPLE CHATBOT DEMO - {self.model_name.upper()}\")\n",
    "        print(f\"MacBook Air | CPU Mode | LangChain Integration\")\n",
    "        print(\"=\"*60)\n",
    "        print(\" COMMANDS:\")\n",
    "        print(\"Type 'demo' - Run 3 sample questions\")\n",
    "        print(\"Type '1-5' - Quick demo questions\")\n",
    "        print(\"Type 'stats' - Show performance\")\n",
    "        print(\"Type 'quit' - Exit\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    def quick_demo(self):\n",
    "        \"\"\"Run 3 pre-defined demo questions (perfect for video)\"\"\"\n",
    "        print(\"\\n RUNNING QUICK DEMO (3 Questions)...\")\n",
    "        demo_samples = self.demo_questions[:3]\n",
    "        \n",
    "        for i, question in enumerate(demo_samples, 1):\n",
    "            print(f\"\\nQ{i}: {question}\")\n",
    "            print(end=\"\", flush=True)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            response = self.chain.run(question)\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            # Simple cleaning\n",
    "            clean_response = response.split(\"Assistant:\")[-1].strip()\n",
    "            if len(clean_response) > 80:\n",
    "                clean_response = clean_response[:80] + \"...\"\n",
    "            \n",
    "            print(f\"Response ({response_time:.1f}s): {clean_response}\")\n",
    "            self.conversation_count += 1\n",
    "        \n",
    "        print(f\"\\nâœ… Demo complete! {self.conversation_count} conversations\")\n",
    "    \n",
    "    def numbered_demo(self, question_num):\n",
    "        \"\"\"Run specific demo question\"\"\"\n",
    "        if 1 <= question_num <= len(self.demo_questions):\n",
    "            question = self.demo_questions[question_num - 1]\n",
    "            print(f\"\\n Question {question_num}: {question}\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            response = self.chain.run(question)\n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            clean_response = response.split(\"Assistant:\")[-1].strip()[:100] + \"...\"\n",
    "            print(f\"({response_time:.1f}s): {clean_response}\")\n",
    "            self.conversation_count += 1\n",
    "        else:\n",
    "            print(\"Invalid question number (1-5)\")\n",
    "    \n",
    "    def show_stats(self):\n",
    "        \"\"\"Simple stats display\"\"\"\n",
    "        print(f\"\\n QUICK STATS\")\n",
    "        print(f\"   Total conversations: {self.conversation_count}\")\n",
    "        print(f\"   Model: {self.model_name}\")\n",
    "        print(f\"   Load time: {load_time:.1f}s\")\n",
    "        print(f\"   Avg response: ~{test_time:.1f}s\")\n",
    "        print(f\"   Memory: ~{model_params * 4:.0f}MB\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Simple main loop - minimal typing\"\"\"\n",
    "        self.show_header()\n",
    "        \n",
    "        print(\"\\n Ready! Type 'demo' to start...\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                user_input = input(\"\\n> \").strip().lower()\n",
    "                \n",
    "                if user_input in ['quit', 'exit', 'q']:\n",
    "                    self.show_final_summary()\n",
    "                    break\n",
    "                \n",
    "                elif user_input == 'demo':\n",
    "                    self.quick_demo()\n",
    "                \n",
    "                elif user_input == 'stats':\n",
    "                    self.show_stats()\n",
    "                \n",
    "                elif user_input in ['1', '2', '3', '4', '5']:\n",
    "                    self.numbered_demo(int(user_input))\n",
    "                \n",
    "                elif user_input:\n",
    "                    # Single word/short query\n",
    "                    print(f\"\\n Custom: {user_input}\")\n",
    "                    start_time = time.time()\n",
    "                    response = self.chain.run(user_input)\n",
    "                    response_time = time.time() - start_time\n",
    "                    \n",
    "                    clean_response = response.split(\"Assistant:\")[-1].strip()[:80] + \"...\"\n",
    "                    print(f\" ({response_time:.1f}s): {clean_response}\")\n",
    "                    self.conversation_count += 1\n",
    "                \n",
    "                else:\n",
    "                    print(\" Try: 'demo', '1', 'stats', or 'quit'\")\n",
    "            \n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\n\\n  Demo interrupted\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\" {str(e)[:40]}\")\n",
    "        \n",
    "        print(\" Demo complete!\")\n",
    "    \n",
    "    def show_final_summary(self):\n",
    "        \"\"\"Clean final summary\"\"\"\n",
    "        print(f\"\\n\" + \"=\"*50)\n",
    "        print(\"=\"*50)\n",
    "        print(f\" Model: {self.model_name} loaded successfully\")\n",
    "        print(f\"LangChain: Chain + Prompt integration\")\n",
    "        print(f\"Chatbot: {self.conversation_count} interactions\")\n",
    "        print(f\" Performance: {load_time:.1f}s load | {test_time:.1f}s response\")\n",
    "        print(f\"Memory: {model_params * 4:.0f}MB\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "\n",
    "\n",
    "# LAUNCH SIMPLIFIED CHATBOT\n",
    "\n",
    "simple_bot = SimpleChatbot(chain, model_name)\n",
    "\n",
    "# Interactive mode\n",
    "simple_bot.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d03cbb7-3784-4a2e-8c38-be410ca82b42",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f40b4f-3ea4-4daf-b970-c2efc299d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Testing Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1f4a6c74-f100-42d3-839c-425a16a3d3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases =[\n",
    "    {\"question\": \"What is the capital of France?\" , \"expected\" : \"Paris\"},\n",
    "    {\"question\": \"What is AI?\", \"expected\": \"artificial\"}\n",
    "]\n",
    "\n",
    "accuracy_results =[]\n",
    "latencies =[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3a20f916-542d-4465-9845-ae440fabe4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are an AI assistant.\n",
      "\n",
      "Instructions:\n",
      "- keep answers clear\n",
      "- Use simple language\n",
      "- Focus on core concepts\n",
      "\n",
      "User Question: What is the capital of France?\n",
      "\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "What is the capital of Fr FALL\n",
      "Latency: 0.68s | Response:  You are an AI assistant.\n",
      "\n",
      "Instructions:\n",
      "- keep answers clea...\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are an AI assistant.\n",
      "\n",
      "Instructions:\n",
      "- keep answers clear\n",
      "- Use simple language\n",
      "- Focus on core concepts\n",
      "\n",
      "User Question: What is AI?\n",
      "\n",
      "Assistant:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "What is AI?               FALL\n",
      "Latency: 0.64s | Response:  You are an AI assistant.\n",
      "\n",
      "Instructions:\n",
      "- keep answers clea...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for case in test_cases:\n",
    "    start_time = time.time()\n",
    "    response = chain.run(question=case[\"question\"])\n",
    "    response_time = time.time()- start_time\n",
    "    \n",
    "    accuracy_pass = case[\"expected\"] in response.lower()\n",
    "    coherence_pass = len(response.split())>5\n",
    "\n",
    "    accuracy_results.append(accuracy_pass)\n",
    "    latencies.append(response_time)\n",
    "\n",
    "    status = \"PASS\" if accuracy_pass and coherence_pass else \"FALL\"\n",
    "    print(f\"{case['question'][:25]:<25} {status}\")\n",
    "    print(f\"Latency: {response_time:.2f}s | Response: {response[:60]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51541fdc-4ae2-4f69-b509-4cc6e2662e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
